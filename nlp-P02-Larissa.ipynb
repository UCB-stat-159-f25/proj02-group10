{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d6c21d-fe08-4c06-abc3-67100121079a",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a8e1e-eac4-49f6-9b00-eeb7287fd561",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "### Process speeches using the SpaCy nlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b5677f-8a3c-4bfb-8ab7-d2fa34855f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from utils import *\n",
    "\n",
    "plt.style.use('seaborn-v0_8-dark')\n",
    "\n",
    "# read in SOTU.csv\n",
    "sou_original = pd.read_csv('./data/SOTU.csv')\n",
    "sou = edit_year(sou_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba6791b-9783-49cc-a355-46d5481a91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69ffb2e-d845-4612-bdad-fa57638bae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the speech dataframe for speeches from 2000 and onwards\n",
    "sou = sou[sou['Year'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e55f9f-7523-4e5b-a378-41a68ab8192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each speech using the 'nlp' function\n",
    "sou_text = sou[\"Text\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60e42e0a-1921-40f8-97df-e6527ab821aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all speeches from => 2000\n",
    "speeches = [token for token in sou_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42e12568-aa6c-41ea-a9a9-465fb70eb951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(speeches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f2522-ca14-4d65-85e7-59d814553be1",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Analyze Tokens vs Lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33a0b0-bcb8-4d99-961d-830f9adec538",
   "metadata": {},
   "source": [
    "#### Token List\n",
    "List of tokens without stop words, punctuation or spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad3703-3d9f-4aac-aa7d-c7d519d01808",
   "metadata": {},
   "source": [
    "- Create a second list of the lemmas for these same tokens.\n",
    "- Display the top 25 for each of these and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3935341-828c-4f29-a471-c6690b77eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    allowed = bool(token\n",
    "                   and not token.is_stop\n",
    "                       and not token.is_punct\n",
    "                           and not token.is_space)\n",
    "    return allowed\n",
    "    \n",
    "token_list = [token for s in speeches for token in s]\n",
    "\n",
    "complete_filtered_tokens = [\n",
    "    token.lower_\n",
    "    for token in token_list\n",
    "    if is_token_allowed(token)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f82d49e3-be5b-4ab6-85e3-af6e843be2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('america', 816),\n",
       " ('people', 637),\n",
       " ('american', 582),\n",
       " ('new', 530),\n",
       " ('years', 439),\n",
       " ('americans', 437),\n",
       " ('world', 425),\n",
       " ('year', 406),\n",
       " ('country', 369),\n",
       " ('jobs', 348),\n",
       " ('tonight', 344),\n",
       " ('work', 324),\n",
       " ('know', 323),\n",
       " ('let', 320),\n",
       " ('congress', 317),\n",
       " ('nation', 311),\n",
       " ('time', 301),\n",
       " ('help', 282),\n",
       " ('need', 266),\n",
       " ('tax', 255)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(complete_filtered_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e857e3-cb61-464a-91cf-7cdef4c0a0dc",
   "metadata": {},
   "source": [
    "#### Lemma List\n",
    "List of lemmas without stop words, punctuation or spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
